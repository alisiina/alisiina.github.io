---
layout: post
title:  Advanced classification methods
date:   2019-02-22
author: Ali Sina
summary: A draft summary and description
mathjax: true
tags: [statistics, machine_learning, statistical_learning, nonlinear, trees, decision_trees, random_forests, bagging, boosting]
postFooter: Additional information, and maybe a <a href="#">link or two</a>.
---

> This is the seventh in a [series](https://alisiina.github.io/2019/01/28/statistical-learning-series.html) of posts that I'm doing on statistical learning. All the material is based on [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) book which was taught by the authors and Stanford University professors Trevor Hastie and Rob Tibshirani. The aim is to condense the concepts taught in the course and the material in the book to a series of under-10-minute reads.

### What

Much of the previous posts in this series are examples of *supervised learning*. This is when we have a response variable $Y$ that we are interested in predicting.


### Why

Part of exploratory data analysis. Hard to assess results since no universally accepted framework, very domain-specific.

Suprevised learning, we can check our the performance of our model with $Y$.



{% include socialsharing.html %}


* * *
##### FOOTNOTES


[^1]: THe tuning paramters of boosting.
[^2]: A hyperplance is a
