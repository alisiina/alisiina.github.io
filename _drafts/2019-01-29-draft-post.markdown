---
layout: post
title:  Draft post
date:   2019-01-29
author: Ali Sina
summary: A draft summary and description
mathjax: true
tags: [statistics, machine_learning, statistical_learning]
postFooter: Additional information, and maybe a <a href="#">link or two</a>.
---

> This is the first in a [series](https://alisiina.github.io/2019/01/28/statistical-learning-series.html) of posts that I'm doing on statistical learning. All the material is based on [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) book which was taught by the authors and Stanford University professors Trevor Hastie and Rob Tibshirani. The aim is to condense the concepts taught in the course and the material in the book to a series of under-10-minute reads.


### What?

Statistical learning is comprised of a vast number of tools and techniques for *understanding data*. The foundation of these tools are...


### Why?

Statistical learning is mainly used for two reasons: *prediction* and *inference*...


### How?

$f$ can be estimated using methods that are either *parametric* or *non-parametric*...


{% include socialsharing.html %}


* * *
##### FOOTNOTES

[^1]: I use the terms "machine learning" and "statistical learning" interchangebly.
[^2]: Another important "tradeoff" is called the Bias-Variance tradeoff. Read [this article](https://scott.fortmann-roe.com/docs/BiasVariance.html) for a comprehensive explanation.
[^3]: The "hat" on top of $\hat{f}$ means that it is an estimated value as opposed to the real value of $f$. This is standard math syntax.
